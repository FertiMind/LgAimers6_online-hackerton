{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import  OrdinalEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv').drop(columns=['ID'])\n",
    "test = pd.read_csv('./test.csv').drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('임신 성공 여부', axis=1)\n",
    "y = train['임신 성공 여부']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    \"시술 시기 코드\",\n",
    "    \"시술 당시 나이\",\n",
    "    \"시술 유형\",\n",
    "    \"특정 시술 유형\",\n",
    "    \"배란 자극 여부\",\n",
    "    \"배란 유도 유형\",\n",
    "    \"단일 배아 이식 여부\",\n",
    "    \"착상 전 유전 검사 사용 여부\",\n",
    "    \"착상 전 유전 진단 사용 여부\",\n",
    "    \"남성 주 불임 원인\",\n",
    "    \"남성 부 불임 원인\",\n",
    "    \"여성 주 불임 원인\",\n",
    "    \"여성 부 불임 원인\",\n",
    "    \"부부 주 불임 원인\",\n",
    "    \"부부 부 불임 원인\",\n",
    "    \"불명확 불임 원인\",\n",
    "    \"불임 원인 - 난관 질환\",\n",
    "    \"불임 원인 - 남성 요인\",\n",
    "    \"불임 원인 - 배란 장애\",\n",
    "    \"불임 원인 - 여성 요인\",\n",
    "    \"불임 원인 - 자궁경부 문제\",\n",
    "    \"불임 원인 - 자궁내막증\",\n",
    "    \"불임 원인 - 정자 농도\",\n",
    "    \"불임 원인 - 정자 면역학적 요인\",\n",
    "    \"불임 원인 - 정자 운동성\",\n",
    "    \"불임 원인 - 정자 형태\",\n",
    "    \"배아 생성 주요 이유\",\n",
    "    \"총 시술 횟수\",\n",
    "    \"클리닉 내 총 시술 횟수\",\n",
    "    \"IVF 시술 횟수\",\n",
    "    \"DI 시술 횟수\",\n",
    "    \"총 임신 횟수\",\n",
    "    \"IVF 임신 횟수\",\n",
    "    \"DI 임신 횟수\",\n",
    "    \"총 출산 횟수\",\n",
    "    \"IVF 출산 횟수\",\n",
    "    \"DI 출산 횟수\",\n",
    "    \"난자 출처\",\n",
    "    \"정자 출처\",\n",
    "    \"난자 기증자 나이\",\n",
    "    \"정자 기증자 나이\",\n",
    "    \"동결 배아 사용 여부\",\n",
    "    \"신선 배아 사용 여부\",\n",
    "    \"기증 배아 사용 여부\",\n",
    "    \"대리모 여부\",\n",
    "    \"PGD 시술 여부\",\n",
    "    \"PGS 시술 여부\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리형 컬럼들을 문자열로 변환\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].astype(str)\n",
    "    test[col] = test[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "X_train_encoded = X.copy()\n",
    "X_train_encoded[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])\n",
    "\n",
    "X_test_encoded = test.copy()\n",
    "X_test_encoded[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    \"임신 시도 또는 마지막 임신 경과 연수\",\n",
    "    \"총 생성 배아 수\",\n",
    "    \"미세주입된 난자 수\",\n",
    "    \"미세주입에서 생성된 배아 수\",\n",
    "    \"이식된 배아 수\",\n",
    "    \"미세주입 배아 이식 수\",\n",
    "    \"저장된 배아 수\",\n",
    "    \"미세주입 후 저장된 배아 수\",\n",
    "    \"해동된 배아 수\",\n",
    "    \"해동 난자 수\",\n",
    "    \"수집된 신선 난자 수\",\n",
    "    \"저장된 신선 난자 수\",\n",
    "    \"혼합된 난자 수\",\n",
    "    \"파트너 정자와 혼합된 난자 수\",\n",
    "    \"기증자 정자와 혼합된 난자 수\",\n",
    "    \"난자 채취 경과일\",\n",
    "    \"난자 해동 경과일\",\n",
    "    \"난자 혼합 경과일\",\n",
    "    \"배아 이식 경과일\",\n",
    "    \"배아 해동 경과일\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "높은 상관관계를 가지는 Feature: {'해동 난자 수_missing', 'IVF 시술 횟수', '착상 전 유전 진단 사용 여부', '미세주입 배아 이식 수_missing', '미세주입된 난자 수_missing', 'IVF 임신 횟수', '저장된 배아 수_missing', 'IVF 출산 횟수', '대리모 여부', '배아 해동 경과일_missing', '기증자 정자와 혼합된 난자 수_missing', '미세주입에서 생성된 배아 수_missing', '배란 유도 유형', '이식된 배아 수_missing', '저장된 신선 난자 수_missing', '난자 채취 경과일_missing', '기증 배아 사용 여부', '미세주입에서 생성된 배아 수', '부부 주 불임 원인', '해동된 배아 수_missing', '총 생성 배아 수_missing', '미세주입 후 저장된 배아 수_missing', '미세주입된 난자 수_log', '파트너 정자와 혼합된 난자 수_missing', '수집된 신선 난자 수_missing', '파트너 정자와 혼합된 난자 수', '혼합된 난자 수_missing'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ 결측 여부 Feature 추가\n",
    "for col in numeric_columns:\n",
    "    X_train_encoded[col + '_missing'] = X_train_encoded[col].isna().astype(int)\n",
    "    X_test_encoded[col + '_missing'] = X_test_encoded[col].isna().astype(int)\n",
    "\n",
    "# zero_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "# X_train_encoded[numeric_columns] = zero_imputer.fit_transform(X_train_encoded[numeric_columns])\n",
    "# X_test_encoded[numeric_columns] = zero_imputer.transform(X_test_encoded[numeric_columns])\n",
    "\n",
    "from numpy import log1p\n",
    "# 🔹 로그 변환 적용 (Skewed Data Handling)\n",
    "skewed_cols = ['총 생성 배아 수', '수집된 신선 난자 수', '저장된 배아 수', '미세주입된 난자 수']\n",
    "\n",
    "for col in skewed_cols:\n",
    "    X_train_encoded[col + '_log'] = log1p(X_train_encoded[col])\n",
    "    X_test_encoded[col + '_log'] = log1p(X_test_encoded[col])\n",
    "\n",
    "# 3️⃣ 80% 이상 결측치가 있는 컬럼 제거\n",
    "missing_ratio = X_train_encoded.isnull().mean()\n",
    "high_missing_columns = missing_ratio[missing_ratio > 0.8].index.tolist()\n",
    "X_train_encoded.drop(columns=high_missing_columns, inplace=True)\n",
    "X_test_encoded.drop(columns=high_missing_columns, inplace=True)\n",
    "\n",
    "# 4️⃣ Feature Engineering (특성 추가)\n",
    "X_train_encoded['배아_생성_효율'] = X_train_encoded['총 생성 배아 수'] / (X_train_encoded['수집된 신선 난자 수'] + 1)\n",
    "X_test_encoded['배아_생성_효율'] = X_test_encoded['총 생성 배아 수'] / (X_test_encoded['수집된 신선 난자 수'] + 1)\n",
    "\n",
    "X_train_encoded['배아_저장_비율'] = X_train_encoded['저장된 배아 수'] / (X_train_encoded['총 생성 배아 수'] + 1)\n",
    "X_test_encoded['배아_저장_비율'] = X_test_encoded['저장된 배아 수'] / (X_test_encoded['총 생성 배아 수'] + 1)\n",
    "\n",
    "X_train_encoded['난자_배아_비율'] = X_train_encoded['미세주입된 난자 수'] / (X_train_encoded['총 생성 배아 수'] + 1)\n",
    "X_test_encoded['난자_배아_비율'] = X_test_encoded['미세주입된 난자 수'] / (X_test_encoded['총 생성 배아 수'] + 1)\n",
    "\n",
    "\n",
    "# 5️⃣ 이상치 처리 (Clip 적용)\n",
    "for col in ['총 생성 배아 수', '수집된 신선 난자 수', '저장된 배아 수']:\n",
    "    X_train_encoded[col] = X_train_encoded[col].clip(lower=1, upper=50)\n",
    "    X_test_encoded[col] = X_test_encoded[col].clip(lower=1, upper=50)\n",
    "\n",
    "# 6️⃣ Feature Scaling 제거 (트리 모델은 불필요)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 상관 행렬 계산\n",
    "corr_matrix = X_train_encoded.corr()\n",
    "\n",
    "# 높은 상관관계(절대값 0.9 이상)를 가지는 변수 찾기\n",
    "high_corr_features = set()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "            colname = corr_matrix.columns[i]\n",
    "            high_corr_features.add(colname)\n",
    "\n",
    "print(\"높은 상관관계를 가지는 Feature:\", high_corr_features)\n",
    "\n",
    "# 제거\n",
    "X_train_encoded.drop(columns=high_corr_features, inplace=True)\n",
    "X_test_encoded.drop(columns=high_corr_features, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optuna Hyperparameter Tuning:   0%|          | 0/50 [00:00<?, ?it/s][I 2025-02-03 02:50:35,854] A new study created in memory with name: no-name-51aafff9-5ef4-4328-9699-59e0bb9016dc\n",
      "[I 2025-02-03 02:51:16,783] Trial 0 finished with value: 0.7373527621262215 and parameters: {'n_estimators': 662, 'max_depth': 9, 'learning_rate': 0.018423208602250003, 'reg_lambda': 3.6293412499217794}. Best is trial 0 with value: 0.7373527621262215.\n",
      "Optuna Hyperparameter Tuning:   2%|▏         | 1/50 [00:40<33:25, 40.93s/it][I 2025-02-03 02:51:38,781] Trial 1 finished with value: 0.7373081160159602 and parameters: {'n_estimators': 298, 'max_depth': 10, 'learning_rate': 0.014541710478097434, 'reg_lambda': 5.205682481281475}. Best is trial 0 with value: 0.7373527621262215.\n",
      "Optuna Hyperparameter Tuning:   0%|          | 0/50 [02:49<?, ?it/s].79s/it]\n",
      "[I 2025-02-03 02:52:36,405] Trial 2 finished with value: 0.6956604172554173 and parameters: {'n_estimators': 825, 'max_depth': 12, 'learning_rate': 0.24263527713372918, 'reg_lambda': 7.695407587194057}. Best is trial 0 with value: 0.7373527621262215.\n",
      "Optuna Hyperparameter Tuning:   6%|▌         | 3/50 [02:00<33:17, 42.50s/it][I 2025-02-03 02:52:47,304] Trial 3 finished with value: 0.7258675517118048 and parameters: {'n_estimators': 158, 'max_depth': 10, 'learning_rate': 0.26731323639471166, 'reg_lambda': 6.6960796733242764}. Best is trial 0 with value: 0.7373527621262215.\n",
      "Optuna Hyperparameter Tuning:   8%|▊         | 4/50 [02:11<23:01, 30.03s/it][I 2025-02-03 02:53:45,777] Trial 4 finished with value: 0.7374363282249528 and parameters: {'n_estimators': 954, 'max_depth': 9, 'learning_rate': 0.011904732122410352, 'reg_lambda': 3.173711081631305}. Best is trial 4 with value: 0.7374363282249528.\n",
      "Optuna Hyperparameter Tuning:  10%|█         | 5/50 [03:09<30:12, 40.28s/it][I 2025-02-03 02:54:19,501] Trial 5 finished with value: 0.7356102500839661 and parameters: {'n_estimators': 613, 'max_depth': 8, 'learning_rate': 0.06231444745085109, 'reg_lambda': 9.69827217580896}. Best is trial 4 with value: 0.7374363282249528.\n",
      "Optuna Hyperparameter Tuning:  12%|█▏        | 6/50 [03:43<27:54, 38.05s/it][I 2025-02-03 02:54:28,749] Trial 6 finished with value: 0.736977043706186 and parameters: {'n_estimators': 145, 'max_depth': 7, 'learning_rate': 0.014238750298852286, 'reg_lambda': 4.415268893290958}. Best is trial 4 with value: 0.7374363282249528.\n",
      "Optuna Hyperparameter Tuning:  14%|█▍        | 7/50 [03:52<20:31, 28.64s/it][I 2025-02-03 02:55:28,607] Trial 7 finished with value: 0.7292995018064781 and parameters: {'n_estimators': 955, 'max_depth': 11, 'learning_rate': 0.043619762919894174, 'reg_lambda': 7.392510081179292}. Best is trial 4 with value: 0.7374363282249528.\n",
      "Optuna Hyperparameter Tuning:  16%|█▌        | 8/50 [04:52<27:00, 38.58s/it][I 2025-02-03 02:56:07,281] Trial 8 finished with value: 0.7341976402888435 and parameters: {'n_estimators': 696, 'max_depth': 8, 'learning_rate': 0.05567185296419221, 'reg_lambda': 1.9885363549026143}. Best is trial 4 with value: 0.7374363282249528.\n",
      "Optuna Hyperparameter Tuning:  18%|█▊        | 9/50 [05:31<26:22, 38.61s/it][I 2025-02-03 02:56:26,398] Trial 9 finished with value: 0.737134034925693 and parameters: {'n_estimators': 422, 'max_depth': 4, 'learning_rate': 0.014304308253458103, 'reg_lambda': 7.411913568791763}. Best is trial 4 with value: 0.7374363282249528.\n",
      "Optuna Hyperparameter Tuning:  20%|██        | 10/50 [05:50<21:43, 32.59s/it][I 2025-02-03 02:57:17,416] Trial 10 finished with value: 0.7390595202896701 and parameters: {'n_estimators': 988, 'max_depth': 6, 'learning_rate': 0.006399267862092527, 'reg_lambda': 1.0522386394689756}. Best is trial 10 with value: 0.7390595202896701.\n",
      "Optuna Hyperparameter Tuning:  22%|██▏       | 11/50 [06:41<24:50, 38.23s/it][I 2025-02-03 02:58:08,775] Trial 11 finished with value: 0.7388189334085874 and parameters: {'n_estimators': 983, 'max_depth': 6, 'learning_rate': 0.005436213620910689, 'reg_lambda': 1.6961475263194616}. Best is trial 10 with value: 0.7390595202896701.\n",
      "Optuna Hyperparameter Tuning:  24%|██▍       | 12/50 [07:32<26:44, 42.22s/it][I 2025-02-03 02:58:49,617] Trial 12 finished with value: 0.7378924671100184 and parameters: {'n_estimators': 839, 'max_depth': 5, 'learning_rate': 0.006251084211713756, 'reg_lambda': 1.040390499896182}. Best is trial 10 with value: 0.7390595202896701.\n",
      "Optuna Hyperparameter Tuning:  26%|██▌       | 13/50 [08:13<25:46, 41.81s/it][I 2025-02-03 02:59:40,661] Trial 13 finished with value: 0.7386983408474395 and parameters: {'n_estimators': 986, 'max_depth': 6, 'learning_rate': 0.005044897853984609, 'reg_lambda': 1.0003733421064875}. Best is trial 10 with value: 0.7390595202896701.\n",
      "Optuna Hyperparameter Tuning:  28%|██▊       | 14/50 [09:04<26:45, 44.60s/it][I 2025-02-03 03:00:22,690] Trial 14 finished with value: 0.739007593169888 and parameters: {'n_estimators': 811, 'max_depth': 6, 'learning_rate': 0.0075087480385859825, 'reg_lambda': 2.654756674391735}. Best is trial 10 with value: 0.7390595202896701.\n",
      "Optuna Hyperparameter Tuning:  30%|███       | 15/50 [09:46<25:33, 43.82s/it][I 2025-02-03 03:00:59,084] Trial 15 finished with value: 0.7378571390842165 and parameters: {'n_estimators': 798, 'max_depth': 4, 'learning_rate': 0.11373408483804932, 'reg_lambda': 2.568378390273409}. Best is trial 10 with value: 0.7390595202896701.\n",
      "Optuna Hyperparameter Tuning:  32%|███▏      | 16/50 [10:23<23:33, 41.59s/it][I 2025-02-03 03:01:25,267] Trial 16 finished with value: 0.7394503017196363 and parameters: {'n_estimators': 476, 'max_depth': 6, 'learning_rate': 0.02605809182175516, 'reg_lambda': 3.7025698387314208}. Best is trial 16 with value: 0.7394503017196363.\n",
      "Optuna Hyperparameter Tuning:  34%|███▍      | 17/50 [10:49<20:19, 36.95s/it][I 2025-02-03 03:01:53,768] Trial 17 finished with value: 0.7390985678396094 and parameters: {'n_estimators': 465, 'max_depth': 7, 'learning_rate': 0.027557093176972036, 'reg_lambda': 4.33263452515897}. Best is trial 16 with value: 0.7394503017196363.\n",
      "Optuna Hyperparameter Tuning:  36%|███▌      | 18/50 [11:17<18:21, 34.41s/it][I 2025-02-03 03:02:22,552] Trial 18 finished with value: 0.739128533962037 and parameters: {'n_estimators': 462, 'max_depth': 7, 'learning_rate': 0.024065763978704583, 'reg_lambda': 5.316577325268539}. Best is trial 16 with value: 0.7394503017196363.\n",
      "Optuna Hyperparameter Tuning:  38%|███▊      | 19/50 [11:46<16:54, 32.72s/it][I 2025-02-03 03:02:48,623] Trial 19 finished with value: 0.7395692025087528 and parameters: {'n_estimators': 487, 'max_depth': 5, 'learning_rate': 0.026396128013431382, 'reg_lambda': 6.060303052762334}. Best is trial 19 with value: 0.7395692025087528.\n",
      "Optuna Hyperparameter Tuning:  40%|████      | 20/50 [12:12<15:21, 30.73s/it][I 2025-02-03 03:03:05,095] Trial 20 finished with value: 0.7394007018462804 and parameters: {'n_estimators': 311, 'max_depth': 5, 'learning_rate': 0.0852655723168669, 'reg_lambda': 6.256822650417265}. Best is trial 19 with value: 0.7395692025087528.\n",
      "Optuna Hyperparameter Tuning:  42%|████▏     | 21/50 [12:29<12:46, 26.45s/it][I 2025-02-03 03:03:21,880] Trial 21 finished with value: 0.7392064127668958 and parameters: {'n_estimators': 326, 'max_depth': 5, 'learning_rate': 0.09809574972255707, 'reg_lambda': 6.201514943533558}. Best is trial 19 with value: 0.7395692025087528.\n",
      "Optuna Hyperparameter Tuning:  44%|████▍     | 22/50 [12:46<10:59, 23.55s/it][I 2025-02-03 03:03:37,916] Trial 22 finished with value: 0.7393925561335359 and parameters: {'n_estimators': 310, 'max_depth': 5, 'learning_rate': 0.031664964380225855, 'reg_lambda': 8.606396403277788}. Best is trial 19 with value: 0.7395692025087528.\n",
      "Optuna Hyperparameter Tuning:  46%|████▌     | 23/50 [13:02<09:34, 21.29s/it][I 2025-02-03 03:04:03,455] Trial 23 finished with value: 0.7392274205336656 and parameters: {'n_estimators': 547, 'max_depth': 4, 'learning_rate': 0.09560019660561961, 'reg_lambda': 6.217922695487155}. Best is trial 19 with value: 0.7395692025087528.\n",
      "Optuna Hyperparameter Tuning:  48%|████▊     | 24/50 [13:27<09:46, 22.57s/it][I 2025-02-03 03:04:14,896] Trial 24 finished with value: 0.738487708686621 and parameters: {'n_estimators': 224, 'max_depth': 5, 'learning_rate': 0.16033626948480853, 'reg_lambda': 4.302838426201321}. Best is trial 19 with value: 0.7395692025087528.\n",
      "Optuna Hyperparameter Tuning:  50%|█████     | 25/50 [13:39<08:00, 19.23s/it][I 2025-02-03 03:04:34,175] Trial 25 finished with value: 0.7394424280928595 and parameters: {'n_estimators': 393, 'max_depth': 5, 'learning_rate': 0.06475811839880417, 'reg_lambda': 5.932291079339549}. Best is trial 19 with value: 0.7395692025087528.\n",
      "Optuna Hyperparameter Tuning:  52%|█████▏    | 26/50 [13:58<07:41, 19.24s/it][I 2025-02-03 03:04:58,666] Trial 26 finished with value: 0.7397474813940252 and parameters: {'n_estimators': 543, 'max_depth': 4, 'learning_rate': 0.04141343625028309, 'reg_lambda': 5.0254908601228925}. Best is trial 26 with value: 0.7397474813940252.\n",
      "Optuna Hyperparameter Tuning:  54%|█████▍    | 27/50 [14:22<07:58, 20.82s/it][I 2025-02-03 03:05:24,098] Trial 27 finished with value: 0.7397750723377555 and parameters: {'n_estimators': 550, 'max_depth': 4, 'learning_rate': 0.04195471561862098, 'reg_lambda': 4.860607358122689}. Best is trial 27 with value: 0.7397750723377555.\n",
      "Optuna Hyperparameter Tuning:  56%|█████▌    | 28/50 [14:48<08:08, 22.20s/it][I 2025-02-03 03:05:49,376] Trial 28 finished with value: 0.7397517137686149 and parameters: {'n_estimators': 555, 'max_depth': 4, 'learning_rate': 0.0398157705548641, 'reg_lambda': 4.905905774148797}. Best is trial 27 with value: 0.7397750723377555.\n",
      "Optuna Hyperparameter Tuning:  58%|█████▊    | 29/50 [15:13<08:05, 23.13s/it][I 2025-02-03 03:06:19,006] Trial 29 finished with value: 0.7397475716075264 and parameters: {'n_estimators': 663, 'max_depth': 4, 'learning_rate': 0.03857372695544873, 'reg_lambda': 4.847393111692977}. Best is trial 27 with value: 0.7397750723377555.\n",
      "Optuna Hyperparameter Tuning:  60%|██████    | 30/50 [15:43<08:21, 25.08s/it][I 2025-02-03 03:06:49,492] Trial 30 finished with value: 0.7393792307759928 and parameters: {'n_estimators': 679, 'max_depth': 4, 'learning_rate': 0.01945837141338953, 'reg_lambda': 3.5463648187210826}. Best is trial 27 with value: 0.7397750723377555.\n",
      "Optuna Hyperparameter Tuning:  62%|██████▏   | 31/50 [16:13<08:27, 26.70s/it][I 2025-02-03 03:07:16,037] Trial 31 finished with value: 0.7398304054054625 and parameters: {'n_estimators': 585, 'max_depth': 4, 'learning_rate': 0.040314325768186714, 'reg_lambda': 4.9869088471461165}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  64%|██████▍   | 32/50 [16:40<07:59, 26.65s/it][I 2025-02-03 03:07:46,709] Trial 32 finished with value: 0.7396976404362081 and parameters: {'n_estimators': 630, 'max_depth': 4, 'learning_rate': 0.05015036536175722, 'reg_lambda': 4.892905358152067}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  66%|██████▌   | 33/50 [17:10<07:53, 27.86s/it][I 2025-02-03 03:08:21,055] Trial 33 finished with value: 0.7397689374431327 and parameters: {'n_estimators': 600, 'max_depth': 4, 'learning_rate': 0.03532372771942896, 'reg_lambda': 5.535694136373442}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  68%|██████▊   | 34/50 [17:45<07:56, 29.80s/it][I 2025-02-03 03:08:58,626] Trial 34 finished with value: 0.7396956204335798 and parameters: {'n_estimators': 740, 'max_depth': 4, 'learning_rate': 0.0339200691305885, 'reg_lambda': 5.584111140419393}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  70%|███████   | 35/50 [18:22<08:02, 32.13s/it][I 2025-02-03 03:09:41,634] Trial 35 finished with value: 0.7334921846956759 and parameters: {'n_estimators': 585, 'max_depth': 12, 'learning_rate': 0.018938919764636745, 'reg_lambda': 6.937306349424977}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  72%|███████▏  | 36/50 [19:05<08:15, 35.40s/it][I 2025-02-03 03:10:17,807] Trial 36 finished with value: 0.7295805231514139 and parameters: {'n_estimators': 573, 'max_depth': 10, 'learning_rate': 0.07554650570693396, 'reg_lambda': 4.137198329416116}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  74%|███████▍  | 37/50 [19:41<07:43, 35.63s/it][I 2025-02-03 03:10:42,954] Trial 37 finished with value: 0.7394995802132596 and parameters: {'n_estimators': 520, 'max_depth': 5, 'learning_rate': 0.04858819057188847, 'reg_lambda': 5.550692695325995}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  76%|███████▌  | 38/50 [20:07<06:29, 32.48s/it][I 2025-02-03 03:11:28,835] Trial 38 finished with value: 0.737902351818099 and parameters: {'n_estimators': 736, 'max_depth': 9, 'learning_rate': 0.010043041243008316, 'reg_lambda': 3.7674685070252827}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  78%|███████▊  | 39/50 [20:52<06:41, 36.50s/it][I 2025-02-03 03:12:08,900] Trial 39 finished with value: 0.7329710838467653 and parameters: {'n_estimators': 627, 'max_depth': 11, 'learning_rate': 0.034366830115278225, 'reg_lambda': 6.606138969554486}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  80%|████████  | 40/50 [21:33<06:15, 37.57s/it][I 2025-02-03 03:12:29,128] Trial 40 finished with value: 0.734268017337655 and parameters: {'n_estimators': 381, 'max_depth': 7, 'learning_rate': 0.1375798785361555, 'reg_lambda': 4.710039711033981}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  82%|████████▏ | 41/50 [21:53<04:51, 32.37s/it][I 2025-02-03 03:12:58,537] Trial 41 finished with value: 0.7397313562816211 and parameters: {'n_estimators': 655, 'max_depth': 4, 'learning_rate': 0.040378920910559175, 'reg_lambda': 4.709149344950596}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  84%|████████▍ | 42/50 [22:22<04:11, 31.48s/it][I 2025-02-03 03:13:25,313] Trial 42 finished with value: 0.7395932873361339 and parameters: {'n_estimators': 596, 'max_depth': 4, 'learning_rate': 0.061775439890587716, 'reg_lambda': 5.5782158303255125}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  86%|████████▌ | 43/50 [22:49<03:30, 30.07s/it][I 2025-02-03 03:13:57,572] Trial 43 finished with value: 0.7395656104923944 and parameters: {'n_estimators': 717, 'max_depth': 4, 'learning_rate': 0.021205700683947875, 'reg_lambda': 5.333899134326435}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  88%|████████▊ | 44/50 [23:21<03:04, 30.73s/it][I 2025-02-03 03:14:22,074] Trial 44 finished with value: 0.7396031069314841 and parameters: {'n_estimators': 503, 'max_depth': 5, 'learning_rate': 0.03403472839539021, 'reg_lambda': 3.1316902711266628}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  90%|█████████ | 45/50 [23:46<02:24, 28.86s/it][I 2025-02-03 03:14:51,841] Trial 45 finished with value: 0.7396500323870029 and parameters: {'n_estimators': 669, 'max_depth': 4, 'learning_rate': 0.04858796324900133, 'reg_lambda': 4.716884768392863}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  92%|█████████▏| 46/50 [24:15<01:56, 29.13s/it][I 2025-02-03 03:15:35,702] Trial 46 finished with value: 0.7394325492962424 and parameters: {'n_estimators': 862, 'max_depth': 6, 'learning_rate': 0.01599003900967131, 'reg_lambda': 4.011470304983046}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  94%|█████████▍| 47/50 [24:59<01:40, 33.55s/it][I 2025-02-03 03:16:11,004] Trial 47 finished with value: 0.7391856964923269 and parameters: {'n_estimators': 762, 'max_depth': 4, 'learning_rate': 0.06874686276056316, 'reg_lambda': 3.2753112712352626}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  96%|█████████▌| 48/50 [25:35<01:08, 34.08s/it][I 2025-02-03 03:16:46,774] Trial 48 finished with value: 0.7389649924714939 and parameters: {'n_estimators': 635, 'max_depth': 5, 'learning_rate': 0.05772981491690438, 'reg_lambda': 5.1087055769267}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning:  98%|█████████▊| 49/50 [26:10<00:34, 34.58s/it][I 2025-02-03 03:17:10,296] Trial 49 finished with value: 0.7391730311726076 and parameters: {'n_estimators': 424, 'max_depth': 6, 'learning_rate': 0.04384291517192496, 'reg_lambda': 7.101891884259034}. Best is trial 31 with value: 0.7398304054054625.\n",
      "Optuna Hyperparameter Tuning: 100%|██████████| 50/50 [26:34<00:00, 31.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Optuna 최적화 완료! 최적 하이퍼파라미터: {'n_estimators': 585, 'max_depth': 4, 'learning_rate': 0.040314325768186714, 'reg_lambda': 4.9869088471461165}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: ROC-AUC = 0.7378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: ROC-AUC = 0.7427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3: ROC-AUC = 0.7399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4: ROC-AUC = 0.7376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross Validation Progress: 100%|██████████| 5/5 [00:20<00:00,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5: ROC-AUC = 0.7411\n",
      "\n",
      "✅ 5-Fold ROC-AUC 점수 평균: 0.7398\n",
      "각 Fold 점수: [0.737842502513753, 0.7427190621099498, 0.7398947613739844, 0.737643518351006, 0.7410521826786194]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from tqdm import tqdm\n",
    "\n",
    "# Optuna 진행 상황을 추적하는 tqdm 콜백 함수\n",
    "class TQDMCallback:\n",
    "    def __init__(self, total):\n",
    "        self.pbar = tqdm(total=total, desc=\"Optuna Hyperparameter Tuning\", position=0, leave=True)\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def close(self):\n",
    "        self.pbar.close()\n",
    "\n",
    "# 하이퍼파라미터 튜닝 (Optuna)\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 10.0),\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_train_encoded, y):\n",
    "        X_train_fold, X_val_fold = X_train_encoded.iloc[train_idx], X_train_encoded.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        y_val_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        auc_score = roc_auc_score(y_val_fold, y_val_pred)\n",
    "        cv_scores.append(auc_score)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Optuna 최적화 실행 (시도 횟수: 50번, tqdm 추가)\n",
    "n_trials = 50\n",
    "tqdm_callback = TQDMCallback(total=n_trials)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=n_trials, callbacks=[tqdm_callback])\n",
    "tqdm_callback.close()\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "best_params = study.best_params\n",
    "print(f\"✅ Optuna 최적화 완료! 최적 하이퍼파라미터: {best_params}\")\n",
    "\n",
    "# 최적 하이퍼파라미터 적용\n",
    "# (XGBoost는 random_state를 명시적으로 다시 지정하는 것이 좋습니다.)\n",
    "model = xgb.XGBClassifier(**best_params, random_state=42)\n",
    "\n",
    "# 교차 검증 설정 (5-Fold)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 진행도 표시를 위한 tqdm 적용\n",
    "cv_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(tqdm(cv.split(X_train_encoded, y), desc=\"Cross Validation Progress\", total=5)):\n",
    "    X_train_fold, X_val_fold = X_train_encoded.iloc[train_idx], X_train_encoded.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_fold,\n",
    "        y_train_fold,\n",
    "        eval_set=[(X_val_fold, y_val_fold)],\n",
    "        verbose=False\n",
    "    )\n",
    "    y_val_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    auc_score = roc_auc_score(y_val_fold, y_val_pred)\n",
    "    cv_scores.append(auc_score)\n",
    "    \n",
    "    print(f\"Fold {fold+1}: ROC-AUC = {auc_score:.4f}\")\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(f\"\\n✅ 5-Fold ROC-AUC 점수 평균: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"각 Fold 점수: {cv_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train_encoded, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = model.predict_proba(X_test_encoded)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('./sample_submission.csv')\n",
    "sample_submission['probability'] = pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('./baseline_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
